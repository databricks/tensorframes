dist: trusty

language: java

jdk: oraclejdk8

sudo: required

cache:
  directories:
  - $HOME/.ivy2/
  - $HOME/.sbt/launchers/
  - $HOME/.cache/spark-versions/
  - $HOME/.sbt/boot/scala-2.10.6/
  - $HOME/.sbt/boot/scala-2.11.8/

env:
  matrix:
  - SCALA_BINARY_VERSION=2.10.6 SPARK_VERSION=2.0.2 SPARK_BUILD="spark-2.0.2-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=2.7.13
  - SCALA_BINARY_VERSION=2.10.6 SPARK_VERSION=2.0.2 SPARK_BUILD="spark-2.0.2-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=3.5.4
  - SCALA_BINARY_VERSION=2.10.6 SPARK_VERSION=2.1.1 SPARK_BUILD="spark-2.1.1-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=2.7.13
  - SCALA_BINARY_VERSION=2.10.6 SPARK_VERSION=2.1.1 SPARK_BUILD="spark-2.1.1-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=3.6.2
  - SCALA_BINARY_VERSION=2.10.6 SPARK_VERSION=2.2.0 SPARK_BUILD="spark-2.2.0-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=2.7.13
  - SCALA_BINARY_VERSION=2.10.6 SPARK_VERSION=2.2.0 SPARK_BUILD="spark-2.2.0-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=3.6.2
  - SCALA_BINARY_VERSION=2.11.8 SPARK_VERSION=2.0.2 SPARK_BUILD="spark-2.0.2-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=2.7.13
  - SCALA_BINARY_VERSION=2.11.8 SPARK_VERSION=2.0.2 SPARK_BUILD="spark-2.0.2-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=3.5.4
  - SCALA_BINARY_VERSION=2.11.8 SPARK_VERSION=2.1.1 SPARK_BUILD="spark-2.1.1-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=2.7.13
  - SCALA_BINARY_VERSION=2.11.8 SPARK_VERSION=2.1.1 SPARK_BUILD="spark-2.1.1-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=3.6.2
  - SCALA_BINARY_VERSION=2.11.8 SPARK_VERSION=2.2.0 SPARK_BUILD="spark-2.2.0-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=2.7.13
  - SCALA_BINARY_VERSION=2.11.8 SPARK_VERSION=2.2.0 SPARK_BUILD="spark-2.2.0-bin-hadoop2.7"
    SPARK_BUILD_URL="http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz"
    TF_PY2_BUILD=tensorflow-1.3.0-cp27-none-linux_x86_64.whl PYTHON_VERSION=3.6.2

before_install:
 - ./bin/download_travis_dependencies.sh

# See this page: http://conda.pydata.org/docs/travis.html
install:
  - sudo apt-get update
  # We do this conditionally because it saves us some downloading if the
  # version is the same.
  - if [[ "$PYTHON_VERSION" == 2.* ]]; then
      wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O miniconda.sh;
      export PYSPARK_PYTHON=python2;
    else
      wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh;
      export PYSPARK_PYTHON=python3;
    fi
  - bash miniconda.sh -b -p $HOME/miniconda
  - export PATH="$HOME/miniconda/bin:$PATH"
  - hash -r
  - conda config --set always_yes yes --set changeps1 no
  - conda update -q conda
  # Useful for debugging any issues with conda
  - conda info -a

  # Create and activate conda environment
  - conda create -q -n test-environment python=$PYTHON_VERSION
  - source activate test-environment
  # Log python & pip versions
  - python --version
  - pip --version
  # Install dependencies from requirements.txt
  - pip install --user -r ./python/requirements.txt
  # Install tensorflow
  - pip install --user tensorflow==1.3

script:
 - rm -rf /home/travis/.javacpp
 # Manually remove previous versions, otherwise the snaphsot will not get updated.
 - rm -rf /home/travis/.ivy2/cache/org.bytedeco.javacpp-presets/*
 # Run the scala unit tests first
 - sbt -Dspark.version=$SPARK_VERSION -Dpython.version=$PYSPARK_PYTHON -Dscala.version=$SCALA_BINARY_VERSION tfs_testing/test
 # Run the python unit tests.
 - sbt -Dspark.version=$SPARK_VERSION -Dscala.version=$SCALA_BINARY_VERSION tfs_testing/assembly
 - SPARK_HOME=$HOME/.cache/spark-versions/$SPARK_BUILD ./python/run-tests.sh
